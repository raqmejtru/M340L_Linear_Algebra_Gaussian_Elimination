---
title: "M340L: Gaussian Elimination Coding Project"
author: 'Table of Contents'
output:
  github_document:
    toc: true
    toc_depth: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Computational: Gaussian Elimination
## Function
The `SimpleEchelonElimination` function outlined below performs Gaussian Elimination for an $m \times n$ matrix with partial pivoting to reduce floating point error.

```{r}
SimpleEchelonElimination = function(x){
  # Define matrix dimensions
  n = ncol(x)
  m = nrow(x)
  
  # Define starting pivot position
  pivot_row = 1
  pivot_col = 1
  
  # Set FLOPs counter to zero
  FLOPs = 0 
  
  # ============================================================================
  # Swap two rows 
  # FLOPs per use of `SwapRows`: 0
  # ============================================================================
  SwapRows = function(x, original_first_row, original_second_row){
    temp_first_row = x[original_first_row, ]
    x[original_first_row, ] = x[original_second_row, ]
    x[original_second_row, ] = temp_first_row
    return(x)
  }
  
  # ============================================================================
  # Row elimination 
  # FLOPs per row use of `EliminateRows`: 2n + 1
  # ============================================================================
  EliminateRows = function(x, pivot_row, pivot_col, eliminated_row){
    # Calculate scaling factor (1 FLOP per row)
    scaling = x[eliminated_row, pivot_col]/x[pivot_row, pivot_col]
    
    # Replace the newly eliminated row (2 FLOPs per element)
    x[eliminated_row, ] = x[eliminated_row, ]  - (scaling * x[pivot_row, ])
    return(x)
  }
  
  # ============================================================================
  # Perform elimination to simple echelon form with partial pivoting
  # ============================================================================
  while (pivot_row <= m & pivot_col <= n) {
    # --------------------------------------------------------------------------
    # Partial pivoting 
    # --------------------------------------------------------------------------
    # Current pivot entry
    current_pivot = x[pivot_row, pivot_col]
    # Subset the pivot column from current pivot row to the bottom of the matrix `x`
    subset_below_pivot = x[pivot_row:m, pivot_col]
    # Value of the entry in subset that contains the largest absolute value
    max_entry_pivot_col = max(abs(subset_below_pivot))
    # Index of the row in the original matrix that contains the entry defined above
    new_pivot_row = which(abs(x[, pivot_col]) == abs(max_entry_pivot_col))
    
    # If current pivot is not the max abs pivot value, swap rows:
    if (current_pivot != max_entry_pivot_col & pivot_row != m & pivot_col != n) {
      x = SwapRows(x, pivot_row, new_pivot_row)
    } 
    
    # --------------------------------------------------------------------------
    # Eliminate rows
    # --------------------------------------------------------------------------
    # Define a counter to loop through rows below the pivot row 
    start_elimination = pivot_row + 1
    
    # As long as the pivot row is not the last row, proceed with row elimination:
    if (pivot_row != m) {
      for (current_row in start_elimination:m) {
        x = EliminateRows(x, pivot_row, pivot_col, current_row)
        
        # Keep track of FLOPs per eliminated row 
        FLOPs = FLOPs + (2*n + 1)
      }
    }
    
    # Continue while loop
    pivot_row = pivot_row + 1
    pivot_col = pivot_col + 1
  }
  
  # Generate output
  output = list(FLOPs = paste0("FLOPs: ", FLOPs), Echelon_Matrix = x)
  
  return(output)
}
```
  
## Test Case: 11 x 10
An $11 \times 10$ `test_matrix` is defined below by sampling entries from a uniform distribution from 0 to 1. 
```{r}
set.seed(1)
test_matrix = matrix(data = runif((11*10), min = 0, max = 1), nrow = 11, ncol = 10)
test_matrix
```

The `SimpleEchelonElimination` function is used to row reduce the `test_matrix` below. 
```{r}
SimpleEchelonElimination(test_matrix)
```
## Other Test Cases
The matrices defined below are used to show that the `SimpleEchelonElimination` function also works for square matrices and matrices with more columns than rows. 
```{r}
square_matrix = matrix(data = runif((6*6), min = 0, max = 1), nrow = 6, ncol = 6)
square_matrix
SimpleEchelonElimination(square_matrix)

test_matrix_2 = matrix(data = runif((4*7), min = 0, max = 1), nrow = 4, ncol = 7)
test_matrix_2
SimpleEchelonElimination(test_matrix_2)
```




# 2. Theoretical: FLOPs

In the Gaussian Elimination function above, the row reduction of an $m \times n$ matrix requires $(\sum_{i=1}^{m-1}i)(2n+1)$ operations.

The Row Elimination portion of the function is copied below with comments describing the number of FLOPs required for each component of the function.

```{r, eval = F}
# Calculate scaling factor ----------------------------------
# 1 FLOP per row due to division
scaling = x[eliminated_row, pivot_col]/x[pivot_row, pivot_col]
    
# Replace the newly eliminated row  -------------------------
# 2 FLOPs per element due to subtraction and multiplication
x[eliminated_row, ] = x[eliminated_row, ]  - (scaling * x[pivot_row, ])
```

Thus, for each row that is input to the elimination function above, $2n + 1$ FLOPs are required.

The total number of FLOPs required in the entire row reduction algorithm is then calculated by multiplying the number of rows that are input to the elimination function by $2n + 1$. There are $\sum_{i=1}^{m-1}i$ rows that are eliminated (under the assumption that there are not any zero entries in the matrix). Thus, this row reduction of an $m \times n$ matrix requires $(\sum_{i=1}^{m-1}i)(2n+1)$ floating point operations.

# 3. Extension: Jacobi Method

## Iterative Scheme

The Jacobi Method is useful for approximating a solution to the problem $Ax = b$ for high-dimensional matrices for which Gaussian Elimination may be inefficient. The method first decomposes an $n \times n$ matrix $A$ into its diagonal component (a new matrix $D$) and its non-diagonal component (a new matrix $N$). The initial problem can now be defined as $(D+N)x = b$.

After algebraic rearrangement, the equation $x=D^{-1}(b-Rx)$ can be used to iteratively estimate $x$, such that $x_{i}^{(k+1)} = \frac{1}{a_{ii}}(b_i - \sum_{j\neq i}^{n} a_{ij} \cdot x_j^k)$, where $x^k$ is the current estimation of $x$, $x^{(k+1)}$ is the next estimation of $x$, and $i = 1, 2, ...n$. This process is repeated until the residuals between $x^k$ and $x^{(k+1)}$ stabilize.

The iterative process must be initialized with a guess of $x$. If information about the system is known, an initial $x^{(0)}$ can be defined as such, otherwise the zero vector is typically used as the initial condition.

## Convergence

This iterative method converges when the matrix $A$ is diagonally dominant such that $|a_{ii}| > \sum_{j\neq i, j=1}^{n} |a_{ij}|$ for all $i$. The residuals of the approximations of $x$ converge in less iterations when the initial guess $x^{(0)}$ is close to the real $x$. The opposite is true when the initial guess is far from the real $x$. The initial $x^{(0)}$ does not affect whether convergence occurs, but rather the speed of convergence. The speed of convergence is also faster for smaller matrices and faster for matrices with smaller conditioning numbers.

The section below specifies characteristics of matrices that prevent the Jacobi Method from converging.\

## Problematic Matrices

The Jacobi Method is ineffective for the following types of matrices:

1.  Matrices with at least one zero diagonal entry are problematic because a zero in the term $a_{i,i}$ in $x_{i}^{(k+1)} = \frac{1}{a_{ii}}(b_i - \sum_{j\neq i}^{n} a_{ij} \cdot x_j^k)$ results in an undefined approximation of $x_i$. This could be addressed by swapping columns so that all diagonal entries of $A$ are non-zero, if possible, while keeping in mind that the matrix should be diagonally dominant.

    $$A_{n,n} =  \begin{pmatrix}  a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\  a_{2,1} & 0 & \cdots & a_{2,n} \\  \vdots  & \vdots  & \ddots & \vdots  \\  a_{n,1} & a_{n,2} & \cdots & a_{n,n}  \end{pmatrix}$$

2.  Matrices that are not diagonally dominant will not converge using the Jacobi Method.

    $$A =  \begin{pmatrix}  
    0.1 & 2 & 3 \\  
    20 & 0.15 & 8 \\  
      7 & 4 & 0.05  \end{pmatrix}$$

3.  Ill-conditioned matrices with a large condition number will not converge using the Jacobi Method.

    $$A =  \begin{pmatrix}  
    0.04 & 0.011 \\  
    0.02 & 0.005 \end{pmatrix}$$

4.  The Jacobi Method cannot approximate solutions for rectangular matrices.

## Jacobi Method vs. Row Reduction

Compared to Gaussian Elimination, the Jacobi Method is a more efficient way to solve the problem $Ax = b$ for an $n \times n$ matrix $A$ that satisfies the convergence criteria above, since the iterative algorithm requires $\mathcal O{(n^2)}$ operations, while row reduction requires $\mathcal O{(n^3)}$ operations.

# References

Kaur H, Kaur K. 2012. Convergence of Jacobi and Gauss-Seidel Method and Error Reduction Factor. IOSR Journal of Mathematics. 2(2): p. 20--23. doi: 10.9790/5728-0222023.

Pyzara A, Bylina B, Bylina J. 2011. The influence of a matrix condition number on iterative methods' convergence. In: 2011 Federated Conference on Computer Science and Information Systems (FedCSIS). p. 459--464. <https://ieeexplore.ieee.org/document/6078297>.

Rapp BE. 2017. Numerical Methods for Linear Systems of Equations. Microfluidics: Modelling, Mechanics and Mathematics. p. 497--535. doi: 10.1016/b978-1-4557-3141-1.50025-3.
